{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visual Similarity.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJ+SS/SnxSLC7KiTnSBQVQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oreolorun/Visual-Similarity/blob/main/Visual_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGkzLk8uLeVY"
      },
      "outputs": [],
      "source": [
        "#  importing libraries \n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "x45QbfneLpC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  building neural network (100px with batchnorm)\n",
        "class CarRecognition_bn100(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "    self.conv2 = nn.Conv2d(32, 32, 3)\n",
        "    self.conv3 = nn.Conv2d(32, 64, 3)\n",
        "    self.conv4 = nn.Conv2d(64, 64, 3)\n",
        "    self.conv5 = nn.Conv2d(64, 128, 3)\n",
        "    self.conv6 = nn.Conv2d(128, 128 ,3)\n",
        "    self.conv7 = nn.Conv2d(128, 128, 3)\n",
        "    self.fc1 = nn.Linear(8192, 514)\n",
        "    self.fc2 = nn.Linear(514, 128)\n",
        "    self.fc3 = nn.Linear(128, 4)\n",
        "    self.pool2 = nn.MaxPool2d(2,2)\n",
        "    self.pool4 = nn.MaxPool2d(2,2)\n",
        "    self.pool7 = nn.MaxPool2d(2,2)\n",
        "    self.batchnorm_conv1 = nn.BatchNorm2d(32)\n",
        "    self.batchnorm_conv2 = nn.BatchNorm2d(32)\n",
        "    self.batchnorm_conv3 = nn.BatchNorm2d(64)\n",
        "    self.batchnorm_conv4 = nn.BatchNorm2d(64)\n",
        "    self.batchnorm_conv5 = nn.BatchNorm2d(128)\n",
        "    self.batchnorm_conv6 = nn.BatchNorm2d(128)\n",
        "    self.batchnorm_conv7 = nn.BatchNorm2d(128)\n",
        "    self.batchnorm_fc1 = nn.BatchNorm1d(514)\n",
        "    self.batchnorm_fc2 = nn.BatchNorm1d(128)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 3, 100, 100).float()\n",
        "    x = F.relu(self.batchnorm_conv1(self.conv1(x)))\n",
        "    x = self.pool2(F.relu(self.batchnorm_conv2(self.conv2(x))))\n",
        "    x = F.relu(self.batchnorm_conv3(self.conv3(x)))\n",
        "    x = self.pool4(F.relu(self.batchnorm_conv4(self.conv4(x))))\n",
        "    x = F.relu(self.batchnorm_conv5(self.conv5(x)))\n",
        "    x = F.relu(self.batchnorm_conv6(self.conv6(x)))\n",
        "    x = self.pool7(F.relu(self.batchnorm_conv7(self.conv7(x))))\n",
        "    x = torch.flatten(x,1)\n",
        "    x = F.relu(self.batchnorm_fc1(self.fc1(x)))\n",
        "    return x"
      ],
      "metadata": {
        "id": "6qXluV2FLrXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  loading model state\n",
        "model = CarRecognition_bn100()\n",
        "model.load_state_dict(torch.load('gdrive/My Drive/Neural Networks/Model_States/CarType100_model_state_1e-2_9ep.pt', map_location=device))"
      ],
      "metadata": {
        "id": "lV0IdDgiMV49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_filenames():\n",
        "  \"\"\"\n",
        "  This function loads 2000 random images from each directory\n",
        "  \"\"\"\n",
        "  dir =  {\n",
        "          'sedan': 'gdrive/My Drive/Datasets/Car_Images/sedans',\n",
        "          'coupe': 'gdrive/My Drive/Datasets/Car_Images/coupes',\n",
        "          'suv': 'gdrive/My Drive/Datasets/Car_Images/suvs',\n",
        "          'truck': 'gdrive/My Drive/Datasets/Car_Images/trucks'\n",
        "          }\n",
        "\n",
        "  all_files = []\n",
        "  selected_files = []\n",
        "  loaded_images = []\n",
        "\n",
        "  print('deriving filenames')\n",
        "  for key, value in tqdm(dir.items()):\n",
        "    files = os.listdir(value)\n",
        "    all_files.append(files)\n",
        "\n",
        "  print('selecting random files')\n",
        "  for file_list in tqdm(all_files):\n",
        "    np.random.shuffle(file_list)\n",
        "    selected = file_list[:2000]\n",
        "    selected_files.extend(selected)\n",
        "\n",
        "  print('loading images')\n",
        "  for f in tqdm(selected_files):\n",
        "    #  deriving filepath\n",
        "    if 'sedan' in f:\n",
        "      path = os.path.join(dir['sedan'], f)\n",
        "    elif 'coupe' in f:\n",
        "      path = os.path.join(dir['coupe'], f)\n",
        "    elif 'suv' in f:\n",
        "      path = os.path.join(dir['suv'], f)\n",
        "    elif 'truck' in f:\n",
        "      path = os.path.join(dir['truck'], f)\n",
        "\n",
        "    #  loading image\n",
        "    try:\n",
        "      image = cv2.imread(path)\n",
        "      image = cv2.resize(image, (100, 100))\n",
        "    except Exception:\n",
        "      pass\n",
        "    \n",
        "    #  saving to list\n",
        "    loaded_images.append([image, f])\n",
        "\n",
        "  return loaded_images"
      ],
      "metadata": {
        "id": "xIHOYf5QL1tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = load_filenames()\n",
        "files = [x for x in files if x[0] is not None]"
      ],
      "metadata": {
        "id": "aGpQ0e2tL79i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  saving image arrays\n",
        "with open('gdrive/My Drive/Datasets/similarity_images.pkl', 'wb') as f:\n",
        "  pickle.dump(files, f)"
      ],
      "metadata": {
        "id": "bQiJWHS-MAFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  loading image array\n",
        "with open('gdrive/My Drive/Datasets/similarity_images.pkl', 'rb') as f:\n",
        "  files = pickle.load(f)"
      ],
      "metadata": {
        "id": "j3hBcHu_MFGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  extracting filenames\n",
        "filenames = [x[1] for x in files]"
      ],
      "metadata": {
        "id": "_tpfksYZMHRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derive_filepaths(file_list):\n",
        "  \"\"\"\n",
        "  This function derives the filepath of the selected images\n",
        "  \"\"\"\n",
        "  dir =  {\n",
        "          'sedan': 'gdrive/My Drive/Datasets/Car_Images/sedans',\n",
        "          'coupe': 'gdrive/My Drive/Datasets/Car_Images/coupes',\n",
        "          'suv': 'gdrive/My Drive/Datasets/Car_Images/suvs',\n",
        "          'truck': 'gdrive/My Drive/Datasets/Car_Images/trucks'\n",
        "          }\n",
        "\n",
        "  all = []\n",
        "\n",
        "  for f in tqdm(file_list):\n",
        "    #  deriving filepath\n",
        "    if 'sedan' in f:\n",
        "      path = os.path.join(dir['sedan'], f)\n",
        "    elif 'coupe' in f:\n",
        "      path = os.path.join(dir['coupe'], f)\n",
        "    elif 'suv' in f:\n",
        "      path = os.path.join(dir['suv'], f)\n",
        "    elif 'truck' in f:\n",
        "      path = os.path.join(dir['truck'], f)\n",
        "  \n",
        "    all.append(path)\n",
        "  return all"
      ],
      "metadata": {
        "id": "hJUtCbVGMKA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  deriving filepaths\n",
        "filepaths = derive_filepaths(filenames)"
      ],
      "metadata": {
        "id": "gE5dLu2LMMHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  copying images\n",
        "destination = 'gdrive/My Drive/Datasets/Car_Images/similarity_images'\n",
        "\n",
        "for i in tqdm(range(len(filepaths))):\n",
        "  shutil.copy(filepaths[i], destination)\n",
        "\n",
        "len(os.listdir('gdrive/My Drive/Datasets/Car_Images/similarity_images'))"
      ],
      "metadata": {
        "id": "CTSpUx9JMPDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  preprocessing image arrays into tensors\n",
        "files = [[img/255, f] for img, f in files]\n",
        "files = [[transforms.ToTensor()(img), f] for img, f in files]"
      ],
      "metadata": {
        "id": "m0fcAQZiMVWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  extracting features\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  files = [[model(img), f] for img, f in tqdm(files)]"
      ],
      "metadata": {
        "id": "00QvS3kmMYiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  saving extracted features\n",
        "with open('gdrive/My Drive/Datasets/similarity_features.pkl', 'wb') as f:\n",
        "  pickle.dump(files, f)"
      ],
      "metadata": {
        "id": "Fl9LBiaxMaoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  loading image features\n",
        "with open('gdrive/My Drive/Datasets/similarity_features.pkl', 'rb') as f:\n",
        "  image_features = pickle.load(f)"
      ],
      "metadata": {
        "id": "qns5p2BIMpX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  deriving similarity scores\n",
        "similarity = [[F.cosine_similarity(files[4879][0], img).item(), f] for img, f in tqdm(files)]"
      ],
      "metadata": {
        "id": "z11TQ52mM1Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  creating a series of scores and filenames\n",
        "scores = [x[0] for x in similarity]\n",
        "f_names = [x[1] for x in similarity]\n",
        "\n",
        "sr = pd.Series(scores, index=f_names)\n",
        "sr = sr.sort_values(ascending=False)\n",
        "sr.head(10)"
      ],
      "metadata": {
        "id": "2XANW3VLM63i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}